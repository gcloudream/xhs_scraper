"""
Enhanced Xiaohongshu Fund Scraper with Advanced Anti-Detection
å¢å¼ºçš„å°çº¢ä¹¦åŸºé‡‘çˆ¬è™« - ä¸“é—¨ç”¨äºæ•™è‚²ç›®çš„çš„åŸºé‡‘çŸ¥è¯†å­¦ä¹ 
"""
import time
import random
import json
import csv
from datetime import datetime
from urllib.parse import quote, urljoin
from typing import List, Dict, Optional
import logging
from pathlib import Path

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.common.exceptions import TimeoutException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager

# Enhanced modules
from anti_detection import EnhancedAntiDetection
from captcha_solver import SmartCaptchaHandler
from adaptive_selectors import AdaptiveSelectorManager, SmartElementExtractor
from session_manager import SessionManager, ProxySessionManager
from config import ScrapingConfig
from logger import ScrapingLogger

logger = ScrapingLogger("enhanced_xiaohongshu_scraper")


class EnhancedXiaohongshuFundScraper:
    """å¢å¼ºçš„å°çº¢ä¹¦åŸºé‡‘çˆ¬è™«"""
    
    def __init__(self, config: ScrapingConfig = None, use_proxy: bool = False, proxy_file: str = None):
        self.config = config or ScrapingConfig()
        self.use_proxy = use_proxy
        self.proxy_file = proxy_file
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.anti_detection = EnhancedAntiDetection()
        self.session_manager = SessionManager()
        self.proxy_manager = ProxySessionManager(proxy_file) if proxy_file else None
        self.captcha_handler = None
        self.selector_manager = AdaptiveSelectorManager()
        self.element_extractor = None
        
        # WebDriverç›¸å…³
        self.driver = None
        self.wait = None
        
        # æ•°æ®å­˜å‚¨
        self.scraped_data = []
        self.session_stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'captchas_solved': 0,
            'posts_scraped': 0,
            'session_rotations': 0
        }
        
        logger.log_session_start(self.config.dict())
    
    def initialize_driver(self) -> bool:
        """åˆå§‹åŒ–WebDriver"""
        try:
            # è®¾ç½®Chromeé€‰é¡¹
            chrome_options = self.anti_detection.setup_chrome_options(self.config.headless)
            
            # ä»£ç†è®¾ç½®
            if self.use_proxy and self.proxy_manager:
                proxy = self.proxy_manager.get_best_proxy()
                if proxy:
                    chrome_options.add_argument(f'--proxy-server={proxy}')
                    logger.info(f"ä½¿ç”¨ä»£ç†: {proxy}")
            
            # åˆ›å»ºWebDriver
            # service = Service(ChromeDriverManager().install())
            service = Service('./drivers/chromedriver.exe')
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # è®¾ç½®ç­‰å¾…å’Œè¡Œä¸ºæ¨¡æ‹Ÿå™¨
            self.wait = WebDriverWait(self.driver, self.config.timeout)
            
            # æ³¨å…¥åæ£€æµ‹è„šæœ¬
            self.anti_detection.inject_anti_detection_scripts(self.driver)
            
            # è®¾ç½®è¡Œä¸ºæ¨¡æ‹Ÿå™¨
            self.anti_detection.setup_behavioral_simulator(self.driver)
            
            # åˆå§‹åŒ–å…¶ä»–ç»„ä»¶
            self.captcha_handler = SmartCaptchaHandler(self.driver)
            self.element_extractor = SmartElementExtractor(self.driver)
            
            # å¯åŠ¨ä¼šè¯
            if not self.session_manager.start_session(self.driver):
                logger.warning("ä¼šè¯å¯åŠ¨å¤±è´¥ï¼Œä½†ç»§ç»­è¿è¡Œ")
            
            logger.info("WebDriveråˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error("WebDriveråˆå§‹åŒ–å¤±è´¥", exception=e)
            return False
    
    def search_fund_content(self, keyword: str) -> List[Dict]:
        """æœç´¢åŸºé‡‘å†…å®¹"""
        logger.info(f"å¼€å§‹æœç´¢å…³é”®è¯: {keyword}")
        
        search_url = f"https://www.xiaohongshu.com/search_result?keyword={quote(keyword)}&source=web_explore_feed"
        posts = []
        
        for attempt in range(self.config.max_retries):
            try:
                logger.info(f"æœç´¢å°è¯• {attempt + 1}/{self.config.max_retries}")
                
                # è®°å½•è¯·æ±‚
                self.session_stats['total_requests'] += 1
                self.session_manager.record_request()
                
                # è·å–æ™ºèƒ½å»¶æ—¶
                delay = self.anti_detection.get_delay('search')
                logger.debug(f"æœç´¢å‰å»¶æ—¶: {delay:.2f}ç§’")
                time.sleep(delay)
                
                # è®¿é—®æœç´¢é¡µé¢
                start_time = time.time()
                self.driver.get(search_url)
                self._wait_for_page_load()
                
                # è®°å½•ä»£ç†å“åº”æ—¶é—´
                if self.proxy_manager:
                    response_time = time.time() - start_time
                    current_proxy = self._get_current_proxy()
                    if current_proxy:
                        self.proxy_manager.record_proxy_result(current_proxy, True, response_time)
                
                # æ£€æŸ¥é‡å®šå‘å’ŒéªŒè¯ç 
                if self._handle_redirects_and_captchas():
                    # æ»šåŠ¨åŠ è½½å†…å®¹
                    self._intelligent_scroll_and_load()
                    
                    # è§£ææœç´¢ç»“æœ
                    posts = self._parse_search_results_enhanced()
                    
                    if posts:
                        logger.info(f"æˆåŠŸè·å– {len(posts)} ä¸ªå¸–å­")
                        self.session_stats['successful_requests'] += 1
                        self.anti_detection.record_request_result(True)
                        break
                    else:
                        logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆå¸–å­")
                else:
                    logger.warning("é¡µé¢åŠ è½½æˆ–éªŒè¯ç å¤„ç†å¤±è´¥")
                
            except Exception as e:
                logger.error(f"æœç´¢å‡ºé”™ (å°è¯• {attempt + 1})", exception=e)
                self.session_stats['failed_requests'] += 1
                self.session_manager.record_request(False)
                self.anti_detection.record_request_result(False)
                
                # è®°å½•ä»£ç†å¤±è´¥
                if self.proxy_manager:
                    current_proxy = self._get_current_proxy()
                    if current_proxy:
                        self.proxy_manager.record_proxy_result(current_proxy, False)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢ä¼šè¯
                if self.session_manager.should_rotate_session():
                    logger.info("è½®æ¢ä¼šè¯...")
                    self._rotate_session()
                
                # å¤±è´¥åçš„æ¢å¤å»¶æ—¶
                recovery_delay = self.anti_detection.get_delay('page_load') * (attempt + 1)
                time.sleep(recovery_delay)
        
        return posts
    
    def _wait_for_page_load(self):
        """ç­‰å¾…é¡µé¢åŠ è½½"""
        try:
            # ç­‰å¾…åŸºæœ¬å…ƒç´ 
            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            
            # ç­‰å¾…JavaScriptå®Œæˆ
            self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
            
            # æ™ºèƒ½ç­‰å¾…åŠ¨æ€å†…å®¹
            time.sleep(self.anti_detection.get_delay('page_load'))
            
        except TimeoutException:
            logger.warning("é¡µé¢åŠ è½½è¶…æ—¶")
    
    def _handle_redirects_and_captchas(self) -> bool:
        """å¤„ç†é‡å®šå‘å’ŒéªŒè¯ç """
        try:
            current_url = self.driver.current_url.lower()
            
            # æ£€æŸ¥ç™»å½•é‡å®šå‘
            if "login" in current_url or "signin" in current_url:
                logger.warning("æ£€æµ‹åˆ°ç™»å½•é‡å®šå‘")
                if not self._handle_login_redirect():
                    return False
            
            # æ£€æŸ¥éªŒè¯ç 
            if self.captcha_handler.solver.detect_and_solve_captcha():
                self.session_stats['captchas_solved'] += 1
                self.anti_detection.record_request_result(True, True)
                logger.info("éªŒè¯ç å¤„ç†æˆåŠŸ")
            
            return True
            
        except Exception as e:
            logger.error("å¤„ç†é‡å®šå‘å’ŒéªŒè¯ç æ—¶å‡ºé”™", exception=e)
            return False
    
    def _handle_login_redirect(self) -> bool:
        """å¤„ç†ç™»å½•é‡å®šå‘"""
        skip_strategies = [
            self._try_skip_login_buttons,
            self._try_direct_navigation,
            self._try_clear_cookies_and_retry
        ]
        
        for strategy in skip_strategies:
            try:
                if strategy():
                    logger.info("æˆåŠŸç»•è¿‡ç™»å½•")
                    return True
            except Exception as e:
                logger.debug(f"ç™»å½•ç»•è¿‡ç­–ç•¥å¤±è´¥: {strategy.__name__} - {e}")
                continue
        
        logger.error("æ‰€æœ‰ç™»å½•ç»•è¿‡ç­–ç•¥éƒ½å¤±è´¥äº†")
        return False
    
    def _try_skip_login_buttons(self) -> bool:
        """å°è¯•ç‚¹å‡»è·³è¿‡ç™»å½•æŒ‰é’®"""
        skip_selectors = [
            "//button[contains(text(), 'è·³è¿‡') or contains(text(), 'ç¨å')]",
            "//a[contains(text(), 'è·³è¿‡') or contains(text(), 'ç¨å')]", 
            "//div[contains(@class, 'close') or contains(@class, 'skip')]//button",
            "//span[contains(@class, 'close')]"
        ]
        
        for selector in skip_selectors:
            try:
                element = self.driver.find_element(By.XPATH, selector)
                if element.is_displayed():
                    element.click()
                    time.sleep(2)
                    return "login" not in self.driver.current_url.lower()
            except:
                continue
        
        return False
    
    def _try_direct_navigation(self) -> bool:
        """å°è¯•ç›´æ¥å¯¼èˆª"""
        try:
            target_url = "https://www.xiaohongshu.com/search_result?keyword=åŸºé‡‘&source=web_explore_feed"
            self.driver.get(target_url)
            self._wait_for_page_load()
            return "login" not in self.driver.current_url.lower()
        except:
            return False
    
    def _try_clear_cookies_and_retry(self) -> bool:
        """å°è¯•æ¸…é™¤cookieså¹¶é‡è¯•"""
        try:
            self.driver.delete_all_cookies()
            time.sleep(2)
            target_url = "https://www.xiaohongshu.com/search_result?keyword=åŸºé‡‘&source=web_explore_feed"
            self.driver.get(target_url)
            self._wait_for_page_load()
            return "login" not in self.driver.current_url.lower()
        except:
            return False
    
    def _intelligent_scroll_and_load(self):
        """æ™ºèƒ½æ»šåŠ¨å’ŒåŠ è½½"""
        logger.info("å¼€å§‹æ™ºèƒ½æ»šåŠ¨åŠ è½½å†…å®¹...")
        
        for scroll_attempt in range(self.config.max_scroll_attempts):
            try:
                # è·å–å½“å‰é¡µé¢é«˜åº¦
                current_height = self.driver.execute_script("return document.body.scrollHeight")
                
                # ä½¿ç”¨è¡Œä¸ºæ¨¡æ‹Ÿå™¨è¿›è¡Œè‡ªç„¶æ»šåŠ¨
                if self.anti_detection.behavioral_simulator:
                    self.anti_detection.behavioral_simulator.natural_scrolling()
                else:
                    # å¤‡ç”¨æ»šåŠ¨æ–¹æ³•
                    scroll_distance = random.randint(400, 800)
                    self.driver.execute_script(f"window.scrollBy(0, {scroll_distance});")
                
                # æ¨¡æ‹Ÿé˜…è¯»æ—¶é—´
                reading_time = self.anti_detection.get_delay('scroll')
                time.sleep(reading_time)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == current_height:
                    logger.info("æ²¡æœ‰æ›´å¤šå†…å®¹åŠ è½½")
                    break
                
                # å°è¯•ç‚¹å‡»"åŠ è½½æ›´å¤š"æŒ‰é’®
                self._try_load_more_button()
                
                # éšæœºé¡µé¢äº¤äº’
                if self.anti_detection.behavioral_simulator and random.random() < 0.3:
                    self.anti_detection.behavioral_simulator.random_page_interaction()
                
            except Exception as e:
                logger.warning(f"æ»šåŠ¨åŠ è½½å‡ºé”™: {e}")
                break
    
    def _try_load_more_button(self):
        """å°è¯•ç‚¹å‡»åŠ è½½æ›´å¤šæŒ‰é’®"""
        load_more_selectors = [
            "//button[contains(text(), 'åŠ è½½æ›´å¤š') or contains(text(), 'æŸ¥çœ‹æ›´å¤š')]",
            "//div[contains(text(), 'åŠ è½½æ›´å¤š') or contains(text(), 'æŸ¥çœ‹æ›´å¤š')]",
            "//button[contains(@class, 'load-more')]",
            "//div[contains(@class, 'load-more')]"
        ]
        
        for selector in load_more_selectors:
            try:
                button = self.driver.find_element(By.XPATH, selector)
                if button.is_displayed():
                    # ä½¿ç”¨JavaScriptç‚¹å‡»é¿å…è¢«æ‹¦æˆª
                    self.driver.execute_script("arguments[0].click();", button)
                    time.sleep(self.anti_detection.get_delay('click'))
                    break
            except:
                continue
    
    def _parse_search_results_enhanced(self) -> List[Dict]:
        """å¢å¼ºçš„æœç´¢ç»“æœè§£æ"""
        logger.info("å¼€å§‹è§£ææœç´¢ç»“æœ...")
        
        # ä½¿ç”¨è‡ªé€‚åº”é€‰æ‹©å™¨æŸ¥æ‰¾å¸–å­å…ƒç´ 
        post_elements = self.selector_manager.find_elements_adaptive(self.driver, 'post', timeout=15)
        
        if not post_elements:
            logger.warning("æœªæ‰¾åˆ°å¸–å­å…ƒç´ ï¼Œå°è¯•å‘ç°æ–°é€‰æ‹©å™¨...")
            # å°è¯•å‘ç°æ–°é€‰æ‹©å™¨
            new_selectors = self.selector_manager.discover_new_selectors(self.driver, 'post')
            if new_selectors:
                logger.info(f"å‘ç° {len(new_selectors)} ä¸ªæ–°é€‰æ‹©å™¨")
                # é‡æ–°å°è¯•æŸ¥æ‰¾
                post_elements = self.selector_manager.find_elements_adaptive(self.driver, 'post', timeout=10)
        
        if not post_elements:
            logger.error("ä»ç„¶æœªæ‰¾åˆ°å¸–å­å…ƒç´ ")
            return []
        
        logger.info(f"æ‰¾åˆ° {len(post_elements)} ä¸ªå¸–å­å…ƒç´ ")
        
        posts = []
        max_posts = min(len(post_elements), self.config.max_posts_per_keyword)
        
        for i, element in enumerate(post_elements[:max_posts]):
            try:
                # ä½¿ç”¨æ™ºèƒ½å…ƒç´ æå–å™¨
                post_data = self.element_extractor.extract_post_data_smart(element)
                
                if post_data and self._is_valid_fund_post(post_data):
                    # æ·»åŠ é¢å¤–ä¿¡æ¯
                    post_data.update({
                        'scraped_time': datetime.now().isoformat(),
                        'keyword': getattr(self, 'current_keyword', ''),
                        'scraper_version': 'enhanced_v2.0'
                    })
                    
                    posts.append(post_data)
                    self.session_stats['posts_scraped'] += 1
                    
                    logger.log_post_scraped(post_data['title'], post_data.get('keyword', ''))
                
                # å®šæœŸè¿›è¡Œäººç±»è¡Œä¸ºæ¨¡æ‹Ÿ
                if i % 5 == 0 and self.anti_detection.behavioral_simulator:
                    self.anti_detection.behavioral_simulator.random_page_interaction()
                    
            except Exception as e:
                logger.error(f"æå–å¸–å­ {i+1} æ•°æ®æ—¶å‡ºé”™", exception=e)
                continue
        
        logger.info(f"æˆåŠŸè§£æ {len(posts)} ä¸ªæœ‰æ•ˆå¸–å­")
        return posts
    
    def _is_valid_fund_post(self, post_data: Dict) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„åŸºé‡‘å¸–å­"""
        if not post_data:
            return False
        
        title = post_data.get('title', '').strip()
        description = post_data.get('description', '').strip()
        
        # åŸºæœ¬éªŒè¯
        if not title or title == "æœªçŸ¥æ ‡é¢˜" or len(title) < self.config.min_title_length:
            return False
        
        # åƒåœ¾å†…å®¹è¿‡æ»¤
        combined_text = f"{title} {description}".lower()
        for spam_keyword in self.config.spam_keywords:
            if spam_keyword in combined_text:
                return False
        
        # åŸºé‡‘ç›¸å…³å†…å®¹éªŒè¯
        has_fund_keyword = any(keyword in combined_text for keyword in self.config.required_keywords)
        return has_fund_keyword
    
    def scrape_multiple_keywords(self, keywords: List[str] = None) -> List[Dict]:
        """çˆ¬å–å¤šä¸ªå…³é”®è¯"""
        if keywords is None:
            keywords = self.config.default_keywords
        
        logger.info(f"å¼€å§‹çˆ¬å– {len(keywords)} ä¸ªå…³é”®è¯")
        
        all_posts = []
        
        for i, keyword in enumerate(keywords):
            try:
                logger.info(f"å¤„ç†å…³é”®è¯ {i+1}/{len(keywords)}: {keyword}")
                self.current_keyword = keyword
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢ä¼šè¯
                if self.session_manager.should_rotate_session():
                    self._rotate_session()
                
                # æœç´¢å†…å®¹
                posts = self.search_fund_content(keyword)
                
                # é™åˆ¶æ¯ä¸ªå…³é”®è¯çš„å¸–å­æ•°é‡
                if len(posts) > self.config.max_posts_per_keyword:
                    posts = posts[:self.config.max_posts_per_keyword]
                
                all_posts.extend(posts)
                
                # å…³é”®è¯é—´çš„æ™ºèƒ½å»¶æ—¶
                if i < len(keywords) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªå…³é”®è¯
                    inter_keyword_delay = self.anti_detection.get_delay('search') * 2
                    logger.info(f"å…³é”®è¯é—´å»¶æ—¶: {inter_keyword_delay:.2f}ç§’")
                    time.sleep(inter_keyword_delay)
                
            except Exception as e:
                logger.error(f"å¤„ç†å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™", exception=e)
                continue
        
        # å»é‡å’Œè´¨é‡è¿‡æ»¤
        unique_posts = self._remove_duplicates_and_filter(all_posts)
        
        logger.info(f"çˆ¬å–å®Œæˆï¼Œæ€»å…±è·å¾— {len(unique_posts)} ä¸ªå”¯ä¸€çš„é«˜è´¨é‡å¸–å­")
        return unique_posts
    
    def _remove_duplicates_and_filter(self, posts: List[Dict]) -> List[Dict]:
        """å»é‡å’Œè´¨é‡è¿‡æ»¤"""
        seen_links = set()
        seen_titles = set()
        unique_posts = []
        
        for post in posts:
            link = post.get('link', '')
            title = post.get('title', '')
            
            # åŸºäºé“¾æ¥å»é‡
            if link and link not in seen_links:
                seen_links.add(link)
                unique_posts.append(post)
            # åŸºäºæ ‡é¢˜å»é‡ï¼ˆå¦‚æœæ²¡æœ‰é“¾æ¥ï¼‰
            elif not link and title and title not in seen_titles:
                seen_titles.add(title)
                unique_posts.append(post)
        
        # æŒ‰è´¨é‡æ’åºï¼ˆå¯ä»¥æ ¹æ®ç‚¹èµæ•°ã€æ ‡é¢˜é•¿åº¦ç­‰ï¼‰
        unique_posts.sort(key=lambda p: len(p.get('title', '')), reverse=True)
        
        return unique_posts
    
    def _rotate_session(self):
        """è½®æ¢ä¼šè¯"""
        try:
            logger.info("å¼€å§‹è½®æ¢ä¼šè¯...")
            self.session_stats['session_rotations'] += 1
            
            # ç»“æŸå½“å‰ä¼šè¯
            self.session_manager.end_session(self.driver)
            
            # é‡æ–°åˆå§‹åŒ–driver
            if self.driver:
                self.driver.quit()
                time.sleep(random.uniform(5, 10))
            
            # é‡æ–°åˆå§‹åŒ–
            if self.initialize_driver():
                logger.info("ä¼šè¯è½®æ¢æˆåŠŸ")
            else:
                logger.error("ä¼šè¯è½®æ¢å¤±è´¥")
                
        except Exception as e:
            logger.error("è½®æ¢ä¼šè¯æ—¶å‡ºé”™", exception=e)
    
    def _get_current_proxy(self) -> Optional[str]:
        """è·å–å½“å‰ä»£ç†"""
        try:
            if self.use_proxy and self.proxy_manager:
                # è¿™é‡Œå¯ä»¥ä»driverä¸­è·å–å½“å‰ä½¿ç”¨çš„ä»£ç†
                # ç”±äºseleniumæ— æ³•ç›´æ¥è·å–ï¼Œè¿™é‡Œè¿”å›æœ€è¿‘é€‰æ‹©çš„ä»£ç†
                return None  # ç®€åŒ–å¤„ç†
        except:
            pass
        return None
    
    def save_results(self, posts: List[Dict]):
        """ä¿å­˜ç»“æœ"""
        if not posts:
            logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # ä¿å­˜ä¸ºCSV
            if self.config.output_csv:
                csv_filename = f"enhanced_fund_posts_{timestamp}.csv"
                self._save_to_csv(posts, csv_filename)
            
            # ä¿å­˜ä¸ºJSON
            if self.config.output_json:
                json_filename = f"enhanced_fund_posts_{timestamp}.json"
                self._save_to_json(posts, json_filename)
            
            # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats_filename = f"scraping_stats_{timestamp}.json"
            self._save_stats(stats_filename)
            
        except Exception as e:
            logger.error("ä¿å­˜ç»“æœæ—¶å‡ºé”™", exception=e)
    
    def _save_to_csv(self, posts: List[Dict], filename: str):
        """ä¿å­˜ä¸ºCSV"""
        output_path = Path(self.config.output_directory) / filename
        
        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
            if posts:
                fieldnames = list(posts[0].keys())
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(posts)
        
        logger.info(f"CSVæ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    def _save_to_json(self, posts: List[Dict], filename: str):
        """ä¿å­˜ä¸ºJSON"""
        output_path = Path(self.config.output_directory) / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)
        
        logger.info(f"JSONæ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    
    def _save_stats(self, filename: str):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        output_path = Path(self.config.output_directory) / filename
        
        stats = {
            'session_stats': self.session_stats,
            'session_manager_stats': self.session_manager.get_session_stats(),
            'selector_stats': self.selector_manager.get_selector_stats(),
            'config': self.config.dict()
        }
        
        if self.proxy_manager:
            stats['proxy_stats'] = self.proxy_manager.get_proxy_stats()
        
        if self.captcha_handler:
            stats['captcha_stats'] = self.captcha_handler.get_captcha_stats()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {output_path}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # ç»“æŸä¼šè¯
            if self.session_manager:
                self.session_manager.end_session(self.driver)
            
            # ä¼˜åŒ–é€‰æ‹©å™¨
            if self.selector_manager:
                self.selector_manager.optimize_selectors()
            
            # å…³é—­driver
            if self.driver:
                self.driver.quit()
            
            # è®°å½•ä¼šè¯ç»“æŸ
            logger.log_session_end()
            
        except Exception as e:
            logger.error("æ¸…ç†èµ„æºæ—¶å‡ºé”™", exception=e)
    
    def print_summary(self, posts: List[Dict]):
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ¯ å¢å¼ºç‰ˆå°çº¢ä¹¦åŸºé‡‘çˆ¬è™« - è¿è¡Œæ‘˜è¦")
        print("="*50)
        
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»å¸–å­æ•°: {len(posts)}")
        print(f"   æˆåŠŸè¯·æ±‚: {self.session_stats['successful_requests']}")
        print(f"   å¤±è´¥è¯·æ±‚: {self.session_stats['failed_requests']}")
        print(f"   éªŒè¯ç è§£å†³: {self.session_stats['captchas_solved']}")
        print(f"   ä¼šè¯è½®æ¢: {self.session_stats['session_rotations']}")
        
        # å…³é”®è¯åˆ†å¸ƒ
        if posts:
            keyword_counts = {}
            for post in posts:
                keyword = post.get('keyword', 'unknown')
                keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
            
            print(f"\nğŸ“ˆ å…³é”®è¯åˆ†å¸ƒ:")
            for keyword, count in keyword_counts.items():
                print(f"   {keyword}: {count}ç¯‡")
        
        # ç¤ºä¾‹å¸–å­
        if posts:
            print(f"\nğŸ“ ç¤ºä¾‹å¸–å­ (å‰3ç¯‡):")
            for i, post in enumerate(posts[:3], 1):
                print(f"   {i}. {post['title'][:50]}...")
                print(f"      ä½œè€…: {post['author']}")
                print(f"      ç‚¹èµ: {post['likes']}")
        
        # æ€§èƒ½ç»Ÿè®¡
        session_stats = self.session_manager.get_session_stats()
        print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æˆåŠŸç‡: {session_stats.get('success_rate', 0):.2%}")
        print(f"   ä¼šè¯æ—¶é•¿: {session_stats.get('session_duration', 0):.0f}ç§’")
        print(f"   å¯ç”¨ä¼šè¯: {session_stats.get('available_sessions', 0)}")
        
        print("="*50)


def main():
    """ä¸»å‡½æ•°"""
    scraper = None
    
    try:
        # åˆ›å»ºé…ç½®
        config = ScrapingConfig(
            headless=False,  # è®¾ç½®ä¸ºTrueå¯æ— å¤´è¿è¡Œ
            max_posts_per_keyword=30,
            max_scroll_attempts=5,
            request_delay_min=3.0,
            request_delay_max=8.0
        )
        
        # åˆå§‹åŒ–çˆ¬è™«
        scraper = EnhancedXiaohongshuFundScraper(
            config=config,
            use_proxy=False,  # è®¾ç½®ä¸ºTrueå¹¶æä¾›proxy_fileä»¥ä½¿ç”¨ä»£ç†
            proxy_file=None   # "proxies.txt"
        )
        
        # åˆå§‹åŒ–WebDriver
        if not scraper.initialize_driver():
            logger.error("WebDriveråˆå§‹åŒ–å¤±è´¥")
            return
        
        logger.info("å¼€å§‹å¢å¼ºç‰ˆåŸºé‡‘å†…å®¹çˆ¬å–...")
        
        # çˆ¬å–å†…å®¹
        # keywords = ["ä»Šæ—¥åŸºé‡‘", "åŸºé‡‘çŸ¥è¯†", "åŸºé‡‘æŠ•èµ„", "åŸºé‡‘ç†è´¢", "åŸºé‡‘å…¥é—¨"]
        keywords = ["ä»Šæ—¥åŸºé‡‘"]
        posts = scraper.scrape_multiple_keywords(keywords)
        
        if posts:
            # ä¿å­˜ç»“æœ
            scraper.save_results(posts)
            
            # æ‰“å°æ‘˜è¦
            scraper.print_summary(posts)
            
        else:
            logger.warning("æœªè·å–åˆ°ä»»ä½•å¸–å­")
    
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error("ç¨‹åºæ‰§è¡Œå‡ºé”™", exception=e)
    finally:
        if scraper:
            scraper.cleanup()


if __name__ == "__main__":
    main()